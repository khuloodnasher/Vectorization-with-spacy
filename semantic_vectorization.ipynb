{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "semantic vectorization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khuloodnasher/Vectorization-with-spacy/blob/main/semantic_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV8dP2wovrbs",
        "outputId": "08775b87-4bf6-49b1-bc48-688f146eca87"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "e7F8XvMAuBgV"
      },
      "source": [
        "# Import spaCy and load the language library\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj4eFMdIuBgW",
        "outputId": "ee7c4e4d-eb6d-4e84-a53f-bc369871dbfc"
      },
      "source": [
        "# collecting the vector component of the word science\n",
        "nlp(u'science').vector"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.6030e-01,  3.6893e-02,  2.1941e-01, -9.9548e-02, -7.4640e-02,\n",
              "        3.9272e-01, -7.1837e-01,  2.4464e-01, -8.8964e-02,  2.4763e+00,\n",
              "       -5.4853e-01, -5.2036e-02,  1.4768e-01,  4.9527e-01, -1.7954e-01,\n",
              "        2.0575e-01,  2.4663e-01,  1.4013e+00, -4.8001e-01, -3.8316e-01,\n",
              "        5.2198e-01, -1.2928e-01,  6.2019e-02,  1.5480e-01,  5.6934e-01,\n",
              "       -3.0260e-01,  1.0634e-01,  2.6798e-01, -5.1659e-03,  3.4128e-01,\n",
              "       -5.2146e-01,  2.4627e-01,  4.2073e-01, -1.3535e-02,  7.3479e-01,\n",
              "       -3.9933e-02, -1.1757e-02,  4.0327e-01, -1.4134e-01,  1.9437e-01,\n",
              "        3.9649e-01, -1.3376e-01, -4.0484e-01,  3.2088e-01,  2.5180e-01,\n",
              "        7.6694e-02,  9.4249e-02, -1.1245e-01, -2.1022e-01, -5.0867e-01,\n",
              "        4.4607e-01, -7.5964e-03,  2.9939e-01, -2.5602e-01, -5.0274e-01,\n",
              "       -4.1465e-01,  2.2768e-01, -4.1404e-01, -1.6366e-01, -1.3608e-01,\n",
              "       -1.3307e-01, -5.3627e-02,  6.2552e-01, -7.3778e-02,  4.8966e-01,\n",
              "       -2.4253e-01, -3.8385e-01, -2.0156e-01,  4.3211e-02,  1.6572e-02,\n",
              "       -2.1803e-01, -6.1490e-01, -1.3150e-01, -2.4474e-01, -2.5167e-02,\n",
              "        6.5707e-01, -9.5978e-02,  4.5935e-01,  1.1010e-01,  8.2390e-01,\n",
              "       -5.2218e-01, -6.8328e-01,  2.5383e-01,  3.8977e-01,  2.5374e-01,\n",
              "       -9.2286e-01, -1.2830e+00, -1.0855e-01, -5.2049e-01,  1.3286e-01,\n",
              "       -2.5398e-01,  5.1866e-02, -4.9461e-01, -1.6515e-01, -2.2692e-01,\n",
              "       -3.6744e-01,  1.4451e-01,  3.5044e-01, -2.6213e-02, -1.7611e-01,\n",
              "        4.7645e-02, -1.5554e-01,  1.7882e-01,  1.6802e-01,  6.8002e-02,\n",
              "       -1.5011e+00, -1.9661e-01,  8.1641e-02, -6.4759e-02, -5.4262e-02,\n",
              "        1.1862e-01,  1.7008e-01, -9.4114e-02, -1.0587e+00,  2.8446e-02,\n",
              "        5.3940e-01, -4.2058e-01, -1.1745e-01,  3.4082e-01, -2.3250e-01,\n",
              "        5.3523e-01, -1.4242e-01,  1.8311e-01,  2.5294e-01,  3.7254e-01,\n",
              "        6.8514e-01,  1.9549e-01,  3.6213e-01,  6.3849e-01,  8.7757e-02,\n",
              "        1.9779e-01, -1.6197e-01,  4.5851e-02, -5.2041e-01,  5.2955e-02,\n",
              "       -1.4642e-01, -1.4673e-01, -9.5170e-03,  3.5062e-01,  5.0627e-02,\n",
              "       -6.8578e-01, -1.1213e-01,  3.4631e-01,  3.8368e-01,  2.8359e-01,\n",
              "       -3.1045e-01, -2.3938e-01, -4.0919e-01, -6.2944e-01, -9.5835e-02,\n",
              "        4.4405e-01, -4.3962e-01,  7.8801e-02,  4.2571e-01,  2.0976e-01,\n",
              "        6.5733e-02, -1.5277e-01, -9.0189e-01,  3.1689e-01,  3.3731e-01,\n",
              "        9.0652e-01, -5.9745e-01,  2.6792e-01,  7.0480e-02, -3.1768e-01,\n",
              "        1.0780e-01, -2.5492e-01, -5.6920e-01,  1.6735e-01,  9.7226e-02,\n",
              "        7.3922e-02, -1.1768e-01,  1.7289e-01, -4.9097e-01,  2.0582e-01,\n",
              "       -1.7098e-02,  1.3766e-01,  8.0877e-02, -1.4878e-02, -4.8219e-01,\n",
              "        1.8154e-01,  3.1158e-01,  4.9131e-01,  4.7461e-02,  2.0959e-01,\n",
              "       -1.7905e-01, -4.4750e-02,  1.3173e-01,  3.8842e-01, -1.9604e-02,\n",
              "       -3.1393e-01, -3.6506e-01, -6.5681e-01,  4.6491e-02, -4.1720e-01,\n",
              "        2.7897e-01,  1.3164e-01, -1.7303e-01,  7.4630e-01,  9.1398e-02,\n",
              "       -5.2191e-01,  2.4694e-03,  1.5099e-01,  5.4087e-01, -1.5279e-01,\n",
              "        4.7377e-02,  1.0102e-02,  2.5490e-01,  4.5261e-01, -5.5536e-04,\n",
              "       -4.9474e-01, -1.4781e-02,  2.9270e-01, -1.1192e+00, -5.0434e-01,\n",
              "       -4.1196e-01, -2.7408e-01,  3.0210e-01, -2.8968e-01, -1.8409e-01,\n",
              "       -2.7783e-01, -1.6132e-01, -3.4228e-01, -5.6827e-01,  7.8474e-01,\n",
              "        2.1584e-01, -1.7312e-01, -1.7983e-01,  3.8934e-01,  3.8110e-01,\n",
              "       -2.4654e-01,  2.3866e-01, -3.5739e-01, -4.9134e-01, -3.7996e-01,\n",
              "        8.0180e-02, -5.1150e-01,  2.7952e-01, -9.5099e-02,  2.1199e-01,\n",
              "       -1.5794e-01, -7.4861e-01, -6.4536e-01, -1.3789e-01,  5.2847e-02,\n",
              "        1.4087e-01, -7.0385e-01, -1.5790e-01, -2.1180e-01,  1.0944e-02,\n",
              "        3.0777e-01,  6.0693e-02, -1.5124e-01, -3.4807e-01,  7.3388e-01,\n",
              "       -4.1446e-01, -3.1412e-01,  4.4759e-02,  1.3446e-01, -3.3184e-01,\n",
              "        2.6226e-01,  3.5356e-01, -3.0004e-01,  2.8373e-01,  5.6391e-01,\n",
              "       -5.1395e-01, -3.3578e-01,  2.7544e-02,  6.5633e-01, -2.6583e-02,\n",
              "       -1.0377e+00,  1.3228e-01,  1.5099e-02,  5.6454e-02,  8.6147e-03,\n",
              "        5.4601e-03, -1.3493e-01, -2.4708e-01, -4.5862e-01, -4.9510e-02,\n",
              "       -1.5387e-01, -3.7809e-01, -2.2860e-01,  4.2497e-02, -2.6729e-01,\n",
              "       -3.3294e-01,  2.4910e-01, -2.3316e-01, -5.9566e-01,  1.0306e-02,\n",
              "        2.9487e-01,  7.4849e-02,  9.2992e-02,  8.8193e-02,  1.2973e-01,\n",
              "       -1.8639e-01,  1.4099e-01,  1.6527e-01, -2.4393e-01, -8.7180e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYvarBy422tb",
        "outputId": "38fc1419-9527-4b9d-bcb2-e16ced4de649"
      },
      "source": [
        "# checking the dimension of the word 'science'\n",
        "nlp(u'science').vector.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbXYbzKQ3I7l",
        "outputId": "84e1eeea-14ef-4afc-cf81-bf1d62d81870"
      },
      "source": [
        "doc=nlp(u'Data Science is growing fast')\n",
        "doc.vector"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.91280210e-01,  1.95112586e-01, -8.65119398e-02, -1.81423590e-01,\n",
              "        2.13843986e-01,  2.10707992e-01, -2.97829993e-02,  4.93666008e-02,\n",
              "        5.98881952e-02,  2.18436003e+00, -2.34377742e-01, -3.58103998e-02,\n",
              "       -8.69866014e-02,  2.00313404e-01, -2.87736595e-01, -1.08859967e-02,\n",
              "        4.33112010e-02,  1.83422017e+00, -2.02500612e-01, -1.23194598e-01,\n",
              "       -6.13041148e-02, -1.35062009e-01, -1.73830196e-01, -1.56994745e-01,\n",
              "        1.55494004e-01,  1.01268128e-01,  7.50223994e-02,  4.29799547e-03,\n",
              "        1.08967423e-01, -2.36519868e-03, -2.55994797e-01,  5.28453998e-02,\n",
              "        4.00835603e-01, -2.33482003e-01,  1.04906008e-01,  9.36781988e-02,\n",
              "        2.12504063e-02,  1.58399999e-01, -1.00996017e-01, -1.08265996e-01,\n",
              "        9.53218117e-02,  2.77699940e-02, -3.07945963e-02, -2.25675944e-02,\n",
              "       -1.63440104e-03,  1.85749605e-01,  3.83486040e-02, -1.08940996e-01,\n",
              "       -1.06291793e-01, -1.92099996e-02,  1.78913977e-02,  4.43851165e-02,\n",
              "       -6.99499995e-02, -1.08295187e-01,  3.59179974e-02, -7.94453919e-02,\n",
              "        5.96901998e-02, -2.23139808e-01,  1.43266603e-01, -2.46078581e-01,\n",
              "        1.12342834e-01,  2.29549199e-01,  1.17678002e-01, -2.90356036e-02,\n",
              "        2.78888196e-01, -3.05568010e-01, -1.19481400e-01,  3.66844013e-02,\n",
              "        1.13296211e-01,  6.18728399e-02, -1.40498817e-01,  7.22984076e-02,\n",
              "        1.60104007e-01, -1.44000009e-01, -1.01177409e-01,  8.98941904e-02,\n",
              "        8.85827988e-02, -9.09180380e-03,  2.24497989e-02,  4.06989992e-01,\n",
              "        8.66359919e-02, -1.02145791e-01,  2.40263984e-01,  1.63713694e-01,\n",
              "        8.99889916e-02, -4.01253998e-01, -4.92250204e-01, -1.53039992e-02,\n",
              "        1.36600016e-02,  4.02540043e-02, -2.21770003e-01,  1.47762209e-01,\n",
              "       -2.13274002e-01, -4.25642021e-02, -1.77980393e-01, -7.70390034e-02,\n",
              "       -1.82811409e-01,  4.27628160e-02,  4.99130078e-02, -1.80840001e-01,\n",
              "       -7.19677955e-02, -1.47588596e-01, -1.13251999e-01,  5.85260093e-02,\n",
              "       -8.54460075e-02, -1.27213204e+00,  1.85919704e-03, -1.93814397e-01,\n",
              "        1.42622203e-01,  5.77259883e-02, -6.06447980e-02, -9.52425972e-02,\n",
              "        5.91951981e-02, -2.88483799e-01,  6.67951927e-02,  2.05895990e-01,\n",
              "       -2.32401997e-01,  3.38440016e-02,  7.02920109e-02, -1.12451002e-01,\n",
              "        2.02277917e-02, -3.30557972e-02,  1.09660001e-02,  1.61308013e-02,\n",
              "        5.80327995e-02,  1.91248000e-01,  1.56442001e-01, -3.03327978e-01,\n",
              "        1.39820799e-01,  2.36622002e-02,  1.14942804e-01, -5.37633970e-02,\n",
              "        7.29582012e-02,  4.24840022e-03,  1.02849007e-01, -2.38119215e-01,\n",
              "       -1.72967806e-01, -5.70047982e-02,  7.80437887e-02,  3.18673998e-02,\n",
              "       -6.83582008e-01,  8.56162086e-02,  3.75975996e-01,  1.22940000e-02,\n",
              "        1.81637947e-02, -2.20786005e-01,  6.08265996e-02, -1.93818599e-01,\n",
              "        1.56159876e-02,  1.30738199e-01,  4.35260031e-03,  5.41580329e-03,\n",
              "        2.91548222e-01,  1.05335400e-01, -1.23952433e-01,  1.31658409e-02,\n",
              "       -1.01396814e-01,  1.63700022e-02, -3.73620018e-02, -1.55353993e-01,\n",
              "        2.59339988e-01, -7.99070075e-02,  1.48045391e-01, -9.98032019e-02,\n",
              "       -4.11345094e-01,  4.66800947e-03,  1.48445398e-01, -2.22670987e-01,\n",
              "        3.66252139e-02, -5.51185980e-02,  5.94395995e-02, -1.61145404e-01,\n",
              "       -1.33834600e-01, -1.10520780e-01, -9.44888070e-02,  1.10247806e-01,\n",
              "        4.24426049e-02, -9.06625986e-02, -1.31234363e-01,  1.42374009e-01,\n",
              "        1.91949800e-01, -3.66560034e-02, -1.09808803e-01, -2.75677983e-02,\n",
              "       -1.69059932e-02, -2.80259997e-02, -2.06877992e-01, -6.14521988e-02,\n",
              "        1.16497967e-02, -1.30163789e-01,  6.96435943e-02, -2.03081772e-01,\n",
              "       -8.89516026e-02,  2.03716204e-01, -8.91000032e-02,  5.36159985e-02,\n",
              "        1.55453399e-01,  7.53159970e-02,  8.23311955e-02,  1.87659398e-01,\n",
              "       -1.18454501e-01, -3.67997944e-01,  1.85938001e-01,  1.62667230e-01,\n",
              "       -5.62459938e-02,  1.05388798e-01, -5.13145998e-02,  1.24643996e-01,\n",
              "        5.80014586e-02, -1.19282268e-01, -3.63889992e-01, -4.34752375e-01,\n",
              "        6.42274022e-02, -3.54008019e-01, -7.84379989e-02, -2.56466806e-01,\n",
              "       -6.32720143e-02,  1.15341999e-01, -1.35979399e-01, -2.04694003e-01,\n",
              "       -9.70420018e-02, -1.29366098e-02, -2.98818864e-04, -9.37328041e-02,\n",
              "        3.00107896e-02,  1.41526014e-01,  6.84459955e-02, -3.10580041e-02,\n",
              "        1.02485396e-01,  6.27311990e-02,  9.08621997e-02,  5.27028367e-02,\n",
              "        4.28859964e-02, -1.36406031e-02, -1.93877012e-01,  8.11030120e-02,\n",
              "       -1.95448801e-01, -1.54176399e-01, -2.14762613e-01,  3.62558030e-02,\n",
              "       -1.60341021e-02, -1.29140213e-01, -1.13192603e-01,  3.57150026e-02,\n",
              "       -6.53403986e-04, -2.22835988e-01, -1.38701394e-01, -1.00160003e-01,\n",
              "        1.12594008e-01,  6.29340038e-02,  1.67339802e-01, -1.08099207e-01,\n",
              "       -8.49679932e-02, -1.31568804e-01,  1.73708797e-01, -1.08351789e-01,\n",
              "        1.68794006e-01, -1.33222062e-02,  2.49600019e-02, -1.30822033e-01,\n",
              "        1.95136711e-01,  2.92886585e-01, -4.36471999e-02, -1.91797808e-01,\n",
              "        1.26247406e-01, -3.62118006e-01, -8.34908038e-02,  2.33139992e-02,\n",
              "        6.42565370e-01,  4.40134220e-02, -1.87886000e-01, -2.59945989e-02,\n",
              "       -1.49358198e-01, -3.61211210e-01,  9.25837383e-02, -9.86721832e-03,\n",
              "       -4.13918011e-02,  1.41385600e-01,  3.76719907e-02,  1.67477995e-01,\n",
              "        1.02883413e-01, -3.06919962e-02, -9.57959741e-02,  9.45912004e-02,\n",
              "       -1.49139762e-03, -1.28302008e-01,  1.70210391e-01, -2.06218600e-01,\n",
              "       -2.02333964e-02, -3.29614788e-01, -1.82869405e-01,  7.59612024e-02,\n",
              "        1.29799798e-01, -9.24113989e-02,  7.94827193e-02, -2.63579935e-03,\n",
              "       -3.39937992e-02, -4.93480079e-02, -2.61958599e-01,  1.24960944e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW-Igq-l3b7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fef185c-b12c-4a39-fe2a-e69e67b9353b"
      },
      "source": [
        "doc.vector.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7zG06qduBgY"
      },
      "source": [
        "What's interesting is that Doc and Span objects themselves have vectors, derived from the averages of individual token vectors. <br>This makes it possible to compare similarities between whole documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj71EeDPuBgY"
      },
      "source": [
        "## Identifying similar vectors\n",
        "The best way to expose vector relationships is through the `.similarity()` method of Doc tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2IVlpV2uBgZ",
        "outputId": "ba3ed676-8c74-4448-f67b-6b99715356be"
      },
      "source": [
        "# Create a three-token Doc object:\n",
        "tokens = nlp(u'Data  Science  Physics ')\n",
        "\n",
        "# Iterate through token combinations:\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Data 1.0\n",
            "Data   0.0\n",
            "Data Science 0.2967368\n",
            "Data   0.0\n",
            "Data Physics 0.19863372\n",
            "  Data 0.0\n",
            "    1.0\n",
            "  Science 0.0\n",
            "    1.0\n",
            "  Physics 0.0\n",
            "Science Data 0.2967368\n",
            "Science   0.0\n",
            "Science Science 1.0\n",
            "Science   0.0\n",
            "Science Physics 0.72971195\n",
            "  Data 0.0\n",
            "    1.0\n",
            "  Science 0.0\n",
            "    1.0\n",
            "  Physics 0.0\n",
            "Physics Data 0.19863372\n",
            "Physics   0.0\n",
            "Physics Science 0.72971195\n",
            "Physics   0.0\n",
            "Physics Physics 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTHNdOrmuBgZ"
      },
      "source": [
        "<font color=green>Note that order doesn't matter. `token1.similarity(token2)` has the same value as `token2.similarity(token1)`.</font>\n",
        "#### To view this as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "Z_Gh0X7MuBgZ",
        "outputId": "79241677-3f00-48e2-d6ee-9c10f22557a6"
      },
      "source": [
        "# For brevity, assign each token a name\n",
        "a,b,c = tokens\n",
        "\n",
        "# Display as a Markdown table (this only works in Jupyter!)\n",
        "from IPython.display import Markdown, display\n",
        "display(Markdown(f'<table><tr><th></th><th>{a.text}</th><th>{b.text}</th><th>{c.text}</th></tr>\\\n",
        "<tr><td>**{a.text}**</td><td>{a.similarity(a):{.4}}</td><td>{b.similarity(a):{.4}}</td><td>{c.similarity(a):{.4}}</td></tr>\\\n",
        "<tr><td>**{b.text}**</td><td>{a.similarity(b):{.4}}</td><td>{b.similarity(b):{.4}}</td><td>{c.similarity(b):{.4}}</td></tr>\\\n",
        "<tr><td>**{c.text}**</td><td>{a.similarity(c):{.4}}</td><td>{b.similarity(c):{.4}}</td><td>{c.similarity(c):{.4}}</td></tr>'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-823e0c3a5d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For brevity, assign each token a name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display as a Markdown table (this only works in Jupyter!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMarkdown\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUNRkiDbuBga"
      },
      "source": [
        "As expected, we see the strongest similarity between \"cat\" and \"pet\", the weakest between \"lion\" and \"pet\", and some similarity between \"lion\" and \"cat\". A word will have a perfect (1.0) similarity with itself.\n",
        "\n",
        "If you're curious, the similarity between \"lion\" and \"dandelion\" is very small:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnmlRb_kuBgb"
      },
      "source": [
        "nlp(u'woman').similarity(nlp(u'girl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nij8VSfuBgb"
      },
      "source": [
        "### Opposites are not necessarily different\n",
        "Words that have opposite meaning, but that often appear in the same *context* may have similar vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WY31mnOuBgb"
      },
      "source": [
        "# Create a three-token Doc object:\n",
        "tokens = nlp(u'like love hate')\n",
        "\n",
        "# Iterate through token combinations:\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPUCCbKeuBgc"
      },
      "source": [
        "## Vector norms\n",
        "It's sometimes helpful to aggregate 300 dimensions into a [Euclidian (L2) norm](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Euclidean_norm), computed as the square root of the sum-of-squared-vectors. This is accessible as the `.vector_norm` token attribute. Other helpful attributes include `.has_vector` and `.is_oov` or *out of vocabulary*.\n",
        "\n",
        "For example, our 685k vector library may not have the word \"[nargle](https://en.wikibooks.org/wiki/Muggles%27_Guide_to_Harry_Potter/Magic/Nargle)\". To test this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSM-G0BluBgc"
      },
      "source": [
        "tokens = nlp(u'dog cat nargle')\n",
        "\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhSn2QEguBgc"
      },
      "source": [
        "Indeed we see that \"nargle\" does not have a vector, so the vector_norm value is zero, and it identifies as *out of vocabulary*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IYXmZtZuBgd"
      },
      "source": [
        "## Vector arithmetic\n",
        "Believe it or not, we can actually calculate new vectors by adding & subtracting related vectors. A famous example suggests\n",
        "<pre>\"king\" - \"man\" + \"woman\" = \"queen\"</pre>\n",
        "Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOOBZMiruBgd"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
        "\n",
        "king = nlp.vocab['king'].vector\n",
        "man = nlp.vocab['man'].vector\n",
        "woman = nlp.vocab['woman'].vector\n",
        "\n",
        "# Now we find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
        "new_vector = king - man + woman\n",
        "computed_similarities = []\n",
        "\n",
        "for word in nlp.vocab:\n",
        "    # Ignore words without vectors and mixed-case words:\n",
        "    if word.has_vector:\n",
        "        if word.is_lower:\n",
        "            if word.is_alpha:\n",
        "                similarity = cosine_similarity(new_vector, word.vector)\n",
        "                computed_similarities.append((word, similarity))\n",
        "\n",
        "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
        "\n",
        "print([w[0].text for w in computed_similarities[:10]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXZqyxGguBgd"
      },
      "source": [
        "So in this case, \"king\" was still closer than \"queen\" to our calculated vector, although \"queen\" did show up!"
      ]
    }
  ]
}